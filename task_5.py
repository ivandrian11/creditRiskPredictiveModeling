# -*- coding: utf-8 -*-
"""Task-5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qRZYUYVkkkcYhfzYk9Qh911_3c122xAT
"""

# import library yang akan digunakan
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE

from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score, classification_report

# atur supaya kolomnya terlihat semua
pd.options.display.max_columns = None

# load dataset
df = pd.read_csv('https://media.githubusercontent.com/media/ivandrian11/dataset/master/loan_data_2007_2014.csv', low_memory=False, index_col=0)

df.head()

# cek informasi dataset
df.info()

"""Dapat terlihat bahwa banyak sekali kolom yang terdapat pada dataset. Selanjutnya akan disaring kembali kolom yang benar-benar dibutuhkan dengan menghilangkan kolom dengan informasi yang kurang dibutuhkan atau kurang relevan untuk model."""

# menghilangkan kolom yang kurang relevan
columns_to_ = ['id', 'member_id', 'sub_grade', 'emp_title', 'url', 'desc', 'title', 'zip_code', 'next_pymnt_d',
                  'recoveries', 'collection_recovery_fee', 'total_rec_prncp', 'total_rec_late_fee', 'desc', 'mths_since_last_record',
                  'mths_since_last_delinq', 'mths_since_last_major_derog', 'annual_inc_joint', 'dti_joint', 'verification_status_joint',
                  'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util',
                  'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl', 'inq_last_12m','policy_code',]
df.drop(columns=columns_to_, inplace=True, axis=1)

df.head()

"""Selanjutnya akan dilakukan pengecekan missing value atau nilai yang kosong pada pada tiap kolom."""

df.isnull().sum()

"""> **Pembahasan:**<br>
Dikarenakan dataset yang digunakan tergolong big data dan jumlah missing value tergolong jumlah yang tidak seberapa maka dapat dilakukan penghapusan terhadap missing value tersebut.
"""

# hapus missing value
df.dropna(inplace=True)

df.head()

# cek informasi dataset kembali
df.info()

"""Selanjutnya akan dilakukan eksplorasi data dengan melihat korelasi antar fitur. Dari korelasi ini dapat dilihat juga jika terdapat fitur collinear. Fitur collinear merupakan fitur yang sangat berkorelasi satu sama lain. Dalam Machine Learning, fitur tersebut dapat menyebabkan penurunan kinerja generalisasi karena memiliki nilai variance yang cukup tinggi dan interpretasi model yang lebih sedikit."""

plt.figure(figsize=(20, 16))
correlation_matrix = df.corr().round(2)
 
sns.heatmap(data=correlation_matrix, annot=True, cmap='Set3_r', linewidths=1)
plt.show()

# fungsi untuk mencari fitur collinear
def check_collinear_features(dataset, threshold):
  col_corr = set()
  corr_matrix = dataset.corr()
  for i in range(len(corr_matrix.columns)):
    for j in range(i):
      if abs(corr_matrix.iloc[i,j]) > threshold:
        colname = corr_matrix.columns[i]
        col_corr.add(colname)
  return col_corr

collinear_features = check_collinear_features(df, 0.8)

collinear_features

"""> **Pembahasan:**<br>
Fitur collinear tersebut akan dihapus menjelang pemodelan nantinya.

Selanjutnya akan dilakukan preprocessing terhadap beberapa kolom sehingga dapat memberikan informasi yang berguna untuk model.
"""

# fungsi untuk mengonversi data kolom term
def term_convert(data, col):
    data[col] = pd.to_numeric(data[col].str.replace(' months', ''))
    
term_convert(df, 'term')

# fungsi untuk mengonversi data kolom emp_length
def emp_length_convert(data, col):
    data[col] = data[col].str.replace('\+ years', '')
    data[col] = data[col].str.replace('< 1 year', str(0))
    data[col] = data[col].str.replace('n/a', str(0))
    data[col] = data[col].str.replace(' years', '')
    data[col] = data[col].str.replace(' year', '')
    data[col] = pd.to_numeric(data[col])
    data[col].fillna(value = 0, inplace = True)

emp_length_convert(df, 'emp_length')

# fungsi untuk mengonversi data kolom yang berkaitan dengan tanggal
def date_columns_convert(data, col):
    today_date = pd.to_datetime('2022-05-14')
    data[col] = pd.to_datetime(data[col], format = "%b-%y")
    data['mths_since_' + col] = round(pd.to_numeric((today_date - data[col]) / np.timedelta64(1, 'M')))
    data['mths_since_' + col] = data['mths_since_' + col].apply(lambda x: data['mths_since_' + col].max() if x < 0 else x)
    data.drop(columns = [col], inplace = True)

date_columns_convert(df, 'issue_d')
date_columns_convert(df, 'earliest_cr_line')
date_columns_convert(df, 'last_pymnt_d')
date_columns_convert(df, 'last_credit_pull_d')

"""Pada dataset terdapat target atau label yang menjadi output dari dataset ini yaitu `loan_status`. Terdapat beberapa kategori yang menjadi nilai unik dari label tersebut yang akan diubah menjadi bentuk biner. Di mana nilai `0` untuk peminjam yang buruk dan `1` untuk peminjam yang baik. Peminjam yang buruk didasarkan dari kategori `Charged Off`, `Default`, dan `Late (31-120 days)`. Kategori lainnya akan dianggap sebagai peminjam yang baik."""

df.loan_status.value_counts()

# buat label yang menyimpan nilai biner
df['label'] = np.where(df.loc[:, 'loan_status'].isin(['Charged Off', 'Default', 'Late (31-120 days)']), 0, 1)

# hapus label yang sebelumnya
df.drop(columns = ['loan_status'], inplace = True)

df.head()

# cek jumlah masing-masing label yang telah terkonversi
df.label.value_counts()

"""> **Pembahasan:**<br>
Perbedaan dari kedua nilai biner dapat dikatakan sebagai inbalanced data. Untuk mengatasinya akan dicoba menggunakan salah satu algoritma data sampling yaitu SMOTE. Sehingga pada pemodelan nanti akan diuji yang menggunakan SMOTE dengan yang tidak.

Terakhir akan dilakukan preprocessing untuk data kategorikal menggunakan Label Encoder karena fitur kategorikal yang ada dapat dikatakan sebagai ordinal data.
"""

cat_col = df.select_dtypes('object').columns
for i in range(len(cat_col)):
  le = LabelEncoder()
  df[cat_col[i]] = le.fit_transform(df[cat_col[i]]) 

df.head()

# pembagian fitur dan label
x = df.iloc[:,:-1]
y = df.iloc[:,-1]

# menghapus fitur collinear
x.drop(collinear_features, axis=1, inplace=True)

x.head()

# membagi data training dan testing
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42)

# melakukan standardisasi untuk menyamakan skala data.
scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

# fungsi untuk membuat model
def modeling(x_train, y_train, x_test, y_test):
  models = []
  models.append(('LR', LogisticRegression(solver='liblinear', max_iter=100000)))
  models.append(('KNN', KNeighborsClassifier()))
  models.append(('DT', DecisionTreeClassifier()))
  models.append(('NB', GaussianNB()))
  models.append(('RF', RandomForestClassifier()))

  acc = pd.DataFrame(columns=['test'], index=['LR','KNN','DT', 'NB', 'RF'])

  for name, model in models:
    print(name)
    model.fit(x_train, y_train)
    predictions = model.predict(x_test)
    acc.loc[name, 'test'] = accuracy_score(y_true=y_test, y_pred=predictions)
    print(classification_report(y_true=y_test, y_pred=predictions))

  fig, ax = plt.subplots(figsize=(8,6))
  acc.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
  ax.grid(zorder=0)

# model tanpa SMOTE
modeling(x_train, y_train, x_test, y_test)

""">**Pembahasan:** <br>
Seperti yang dapat dilihat dari hasil di atas, model Tree berkinerja jauh lebih baik dibanding model yang lain. Memang, algoritma berbasis Tree menawarkan stabilitas dan kemampuan interpretasi yang tinggi untuk prediksi pemodelan. Algoritma tersebut memetakan interaksi nonlinier dengan cukup baik, tidak seperti model linier. Algoritma tersebut dapat beradaptasi dengan situasi apa pun dan memecahkan tantangan apa pun (klasifikasi atau regresi).

Selanjutnya dilakukan pemodelan menggunakan data sampling SMOTE.
"""

print("Sebelum OverSampling, jumlah label '1': {}".format(sum(y_train == 1)))
print("Sebelum OverSampling, jumlah label '0': {} \n".format(sum(y_train == 0)))

print('Sebelum OverSampling, ukuran train_X: {}'.format(x_train.shape))
print('Sebelum OverSampling, ukuran train_y: {} \n'.format(y_train.shape))

sm = SMOTE(random_state = 42)
x_train_res, y_train_res = sm.fit_resample(x_train, y_train)

print('Sesudah OverSampling, ukuran train_X: {}'.format(x_train_res.shape))
print('Sesudah OverSampling, ukuran train_y: {} \n'.format(y_train_res.shape))

print("Sesudah OverSampling, jumlah label '1': {}".format(sum(y_train_res == 1)))
print("Sesudah OverSampling, jumlah label '0': {}".format(sum(y_train_res == 0)))

# model dengan SMOTE
modeling(x_train_res, y_train_res, x_test, y_test)

""">**Pembahasan:** <br>
Hasilnya tidak berbeda secara signifikan ya bahkan memang sedikit lebih baik akurasi dari pemodelan tanpa data samping.
"""